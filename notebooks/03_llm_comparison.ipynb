{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fj690mmav4r",
   "source": "# LLM Model Comparison\n\nThis notebook analyses the benchmark results from our LLM comparison framework.\nWe compare four models across multiple dimensions:\n\n| Model | Type | Hosting | Parameters |\n|-------|------|---------|------------|\n| **Mistral 7B** | Open-source | LocalAI (self-hosted) | 7B |\n| **LLaMA 3 8B** | Open-source | LocalAI (self-hosted) | 8B |\n| **Phi-3 Mini** | Open-source | LocalAI (self-hosted) | 3.8B |\n| **GPT-4o-mini** | Proprietary | OpenAI API (external) | N/A |\n\n**Metrics measured:**\n- **Latency**: Total response time, time to first token, tokens per second\n- **Quality**: Relevance, completeness, coherence (scored 0-1)\n- **Turkish language support**: Quality of Turkish-language responses\n- **Memory usage**: RAM delta during inference\n\n> Benchmark data is loaded from `llm-comparison/benchmark_results.json`,\n> generated by `compare_models.py` running against a 3-node k3s cluster with 32GB RAM.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gcvnmkvc1g8",
   "source": "\"\"\"\nImport Dependencies\n-------------------\nWe use json for loading benchmark data and basic Python for tabular display.\nmatplotlib is optional -- if available, we generate charts; otherwise we\nfall back to text-based tables.\n\"\"\"\n\nimport json\nimport os\nfrom pathlib import Path\n\n# Optional: matplotlib for charts\ntry:\n    import matplotlib.pyplot as plt\n    import matplotlib\n    matplotlib.rcParams[\"figure.figsize\"] = (12, 6)\n    matplotlib.rcParams[\"font.size\"] = 11\n    MATPLOTLIB_AVAILABLE = True\n    print(\"matplotlib available -- charts will be rendered\")\nexcept ImportError:\n    MATPLOTLIB_AVAILABLE = False\n    print(\"matplotlib not available -- using text tables\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8whzg4m12oy",
   "source": "## Loading Benchmark Results\n\nThe benchmark results are stored in `llm-comparison/benchmark_results.json`.\nThis file is generated by `compare_models.py` which sends standardised prompts\n(English and Turkish) to each model via their OpenAI-compatible APIs and records\nlatency, throughput, quality metrics, and memory usage.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9jvob66sxy",
   "source": "# ---------------------------------------------------------------------------\n# Load benchmark_results.json\n# ---------------------------------------------------------------------------\n\n# Path relative to the project root\nBENCHMARK_PATH = Path(\"../llm-comparison/benchmark_results.json\")\n\n# Also try absolute path if relative doesn't work\nif not BENCHMARK_PATH.exists():\n    BENCHMARK_PATH = Path(\"/home/homedevlab/kubernetes/kubernetes/ai-stack-k8s/llm-comparison/benchmark_results.json\")\n\nwith open(BENCHMARK_PATH, \"r\", encoding=\"utf-8\") as f:\n    benchmark_data = json.load(f)\n\n# Extract metadata and model results\nmetadata = benchmark_data[\"benchmark_metadata\"]\nmodels = benchmark_data[\"models\"]\n\nprint(\"Benchmark Metadata:\")\nprint(f\"  Timestamp: {metadata['timestamp']}\")\nprint(f\"  Tool:      {metadata['tool']}\")\nif \"notes\" in metadata:\n    print(f\"  Notes:     {metadata['notes']}\")\nprint(f\"\\nModels loaded: {len(models)}\")\nfor m in models:\n    hosting = \"External API\" if m[\"is_external\"] else \"Self-hosted (LocalAI)\"\n    print(f\"  - {m['model_name']} ({m['model_id']}) -- {hosting}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fv6a12uz8r",
   "source": "## Latency Comparison\n\nLatency is critical for interactive applications. We measure three dimensions:\n- **Total response time** -- End-to-end time from request to complete response\n- **Time to first token (TTFT)** -- How quickly the model starts streaming\n- **Tokens per second** -- Throughput of token generation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mc3oz809bmq",
   "source": "# ---------------------------------------------------------------------------\n# Latency Comparison Table\n# ---------------------------------------------------------------------------\n\ndef print_latency_table(models: list[dict]) -> None:\n    \"\"\"Display a formatted latency comparison table.\"\"\"\n    print(\"\\nLatency Comparison\")\n    print(\"=\" * 85)\n    header = (\n        f\"{'Model':<18} {'Avg Total (s)':>14} {'TTFT (s)':>10} \"\n        f\"{'Tok/s':>8} {'Memory (MB)':>12} {'Errors':>8}\"\n    )\n    print(header)\n    print(\"-\" * 85)\n\n    for m in models:\n        ttft = m.get(\"avg_first_token_seconds\")\n        ttft_str = f\"{ttft:.3f}\" if ttft else \"N/A\"\n        mem_delta = m.get(\"memory_delta_used_mb\", 0)\n\n        print(\n            f\"{m['model_name']:<18} \"\n            f\"{m['avg_total_seconds']:>13.3f}s \"\n            f\"{ttft_str:>10} \"\n            f\"{m['avg_tokens_per_second']:>7.1f} \"\n            f\"{mem_delta:>11.1f} \"\n            f\"{m['error_count']:>8}\"\n        )\n\n    print(\"=\" * 85)\n\n    # Identify the fastest and slowest\n    sorted_by_speed = sorted(models, key=lambda x: x[\"avg_total_seconds\"])\n    fastest = sorted_by_speed[0]\n    slowest = sorted_by_speed[-1]\n\n    sorted_by_tps = sorted(models, key=lambda x: x[\"avg_tokens_per_second\"], reverse=True)\n    highest_tps = sorted_by_tps[0]\n\n    print(f\"\\nFastest overall:     {fastest['model_name']} ({fastest['avg_total_seconds']:.3f}s)\")\n    print(f\"Highest throughput:  {highest_tps['model_name']} ({highest_tps['avg_tokens_per_second']:.1f} tok/s)\")\n    print(f\"Slowest overall:     {slowest['model_name']} ({slowest['avg_total_seconds']:.3f}s)\")\n\n\nprint_latency_table(models)\n\n# Optional: matplotlib chart\nif MATPLOTLIB_AVAILABLE:\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    names = [m[\"model_name\"] for m in models]\n    colors = [\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\"]\n\n    # Chart 1: Average total time\n    times = [m[\"avg_total_seconds\"] for m in models]\n    axes[0].barh(names, times, color=colors)\n    axes[0].set_xlabel(\"Seconds\")\n    axes[0].set_title(\"Average Response Time\")\n    axes[0].invert_yaxis()\n\n    # Chart 2: Time to first token\n    ttfts = [m.get(\"avg_first_token_seconds\", 0) or 0 for m in models]\n    axes[1].barh(names, ttfts, color=colors)\n    axes[1].set_xlabel(\"Seconds\")\n    axes[1].set_title(\"Time to First Token\")\n    axes[1].invert_yaxis()\n\n    # Chart 3: Tokens per second\n    tps = [m[\"avg_tokens_per_second\"] for m in models]\n    axes[2].barh(names, tps, color=colors)\n    axes[2].set_xlabel(\"Tokens/sec\")\n    axes[2].set_title(\"Token Throughput\")\n    axes[2].invert_yaxis()\n\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wssivwxbk6",
   "source": "## Quality Scores\n\nResponse quality is evaluated across three dimensions using a heuristic scoring\nsystem implemented in `metrics/response_quality.py`:\n\n- **Relevance** -- Does the response address the question?\n- **Completeness** -- Does it cover all key aspects?\n- **Coherence** -- Is it well-structured and logically consistent?\n\nThe overall quality score is a weighted average of these three dimensions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ysvskg9uibn",
   "source": "# ---------------------------------------------------------------------------\n# Quality Score Comparison\n# ---------------------------------------------------------------------------\n\ndef print_quality_table(models: list[dict]) -> None:\n    \"\"\"Display quality scores broken down by English and Turkish.\"\"\"\n    print(\"\\nQuality Score Comparison (0.0 - 1.0)\")\n    print(\"=\" * 75)\n    header = (\n        f\"{'Model':<18} {'Overall':>10} {'English':>10} {'Turkish':>10} \"\n        f\"{'EN-TR Gap':>10}\"\n    )\n    print(header)\n    print(\"-\" * 75)\n\n    for m in models:\n        overall = m[\"avg_quality_overall\"]\n        en = m[\"avg_quality_en\"]\n        tr = m[\"avg_quality_tr\"]\n        gap = en - tr\n\n        # Quality rating\n        if overall >= 0.8:\n            rating = \"Excellent\"\n        elif overall >= 0.7:\n            rating = \"Good\"\n        elif overall >= 0.6:\n            rating = \"Fair\"\n        else:\n            rating = \"Poor\"\n\n        print(\n            f\"{m['model_name']:<18} \"\n            f\"{overall:>9.4f} \"\n            f\"{en:>9.4f} \"\n            f\"{tr:>9.4f} \"\n            f\"{gap:>+9.4f}\"\n        )\n\n    print(\"=\" * 75)\n\n    # Find best quality model\n    best = max(models, key=lambda x: x[\"avg_quality_overall\"])\n    print(f\"\\nHighest overall quality: {best['model_name']} ({best['avg_quality_overall']:.4f})\")\n\n    # Find the model with smallest EN-TR gap\n    smallest_gap = min(models, key=lambda x: abs(x[\"avg_quality_en\"] - x[\"avg_quality_tr\"]))\n    gap_val = abs(smallest_gap[\"avg_quality_en\"] - smallest_gap[\"avg_quality_tr\"])\n    print(f\"Most consistent (EN/TR): {smallest_gap['model_name']} (gap: {gap_val:.4f})\")\n\n\nprint_quality_table(models)\n\n# Per-prompt quality breakdown for the top model\nprint(\"\\n\\nPer-Prompt Quality Breakdown (Top Model: GPT-4o-mini)\")\nprint(\"-\" * 70)\ngpt4_mini = next((m for m in models if m[\"model_id\"] == \"gpt-4o-mini\"), None)\nif gpt4_mini:\n    for pr in gpt4_mini[\"prompt_results\"]:\n        quality = pr.get(\"quality\", {})\n        print(\n            f\"  [{pr['language'].upper()}] {pr['prompt_id']:<20} \"\n            f\"relevance={quality.get('relevance', 0):.3f}  \"\n            f\"completeness={quality.get('completeness', 0):.3f}  \"\n            f\"coherence={quality.get('coherence', 0):.3f}  \"\n            f\"overall={quality.get('overall', 0):.3f}\"\n        )\n\n# Optional: matplotlib chart\nif MATPLOTLIB_AVAILABLE:\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    names = [m[\"model_name\"] for m in models]\n    x_pos = range(len(names))\n    bar_width = 0.3\n\n    en_scores = [m[\"avg_quality_en\"] for m in models]\n    tr_scores = [m[\"avg_quality_tr\"] for m in models]\n    overall_scores = [m[\"avg_quality_overall\"] for m in models]\n\n    bars1 = ax.bar([x - bar_width for x in x_pos], en_scores, bar_width,\n                   label=\"English\", color=\"#4C72B0\")\n    bars2 = ax.bar(x_pos, tr_scores, bar_width,\n                   label=\"Turkish\", color=\"#C44E52\")\n    bars3 = ax.bar([x + bar_width for x in x_pos], overall_scores, bar_width,\n                   label=\"Overall\", color=\"#55A868\")\n\n    ax.set_xlabel(\"Model\")\n    ax.set_ylabel(\"Quality Score\")\n    ax.set_title(\"Response Quality by Language\")\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(names)\n    ax.legend()\n    ax.set_ylim(0, 1.0)\n    ax.axhline(y=0.7, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Good threshold\")\n\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lqkhfi4c7id",
   "source": "## Turkish Language Support\n\nTurkish language quality is particularly important for this project. We evaluate\nhow well each model handles Turkish prompts, looking at:\n\n- **Turkish quality score** -- A dedicated metric assessing grammar, vocabulary,\n  and use of proper Turkish characters (e.g., ş, ğ, ü, ö, ç, ı)\n- **Language fidelity** -- Whether the model responds in Turkish when prompted in Turkish\n- **EN-TR quality gap** -- How much quality degrades compared to English responses",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ihusqdx87xj",
   "source": "# ---------------------------------------------------------------------------\n# Turkish Language Analysis\n# ---------------------------------------------------------------------------\n\ndef analyse_turkish_support(models: list[dict]) -> None:\n    \"\"\"Deep-dive into Turkish language performance across models.\"\"\"\n    print(\"\\nTurkish Language Support Analysis\")\n    print(\"=\" * 80)\n\n    for m in models:\n        print(f\"\\n--- {m['model_name']} ---\")\n\n        # Filter Turkish prompt results\n        tr_prompts = [\n            pr for pr in m[\"prompt_results\"]\n            if pr.get(\"language\") == \"tr\"\n        ]\n\n        if not tr_prompts:\n            print(\"  No Turkish prompts found in benchmark results.\")\n            continue\n\n        for pr in tr_prompts:\n            quality = pr.get(\"quality\", {})\n            tr_quality = quality.get(\"turkish_quality\")\n            overall = quality.get(\"overall\", 0)\n            response_preview = pr.get(\"response\", \"\")[:150]\n\n            print(f\"  Prompt:  {pr['prompt_id']}\")\n            print(f\"  Quality: overall={overall:.3f}\", end=\"\")\n            if tr_quality is not None:\n                print(f\"  turkish_quality={tr_quality:.3f}\", end=\"\")\n            print()\n            print(f\"  Response preview: {response_preview}...\")\n            print()\n\n    # Summary table: Turkish-specific scores\n    print(\"\\nTurkish Quality Summary\")\n    print(\"-\" * 65)\n    print(f\"{'Model':<18} {'TR Overall':>12} {'TR Quality':>12} {'EN-TR Gap':>12}\")\n    print(\"-\" * 65)\n\n    for m in models:\n        tr_prompts = [\n            pr for pr in m[\"prompt_results\"]\n            if pr.get(\"language\") == \"tr\"\n        ]\n        if tr_prompts:\n            # Average turkish_quality across TR prompts\n            tr_qual_scores = [\n                pr[\"quality\"][\"turkish_quality\"]\n                for pr in tr_prompts\n                if pr.get(\"quality\", {}).get(\"turkish_quality\") is not None\n            ]\n            avg_tr_qual = sum(tr_qual_scores) / len(tr_qual_scores) if tr_qual_scores else 0\n            gap = m[\"avg_quality_en\"] - m[\"avg_quality_tr\"]\n\n            print(\n                f\"{m['model_name']:<18} \"\n                f\"{m['avg_quality_tr']:>11.4f} \"\n                f\"{avg_tr_qual:>11.4f} \"\n                f\"{gap:>+11.4f}\"\n            )\n\n    print(\"-\" * 65)\n    print(\"\\nKey observations:\")\n\n    # Identify best Turkish model\n    best_tr = max(models, key=lambda x: x[\"avg_quality_tr\"])\n    worst_tr = min(models, key=lambda x: x[\"avg_quality_tr\"])\n    print(f\"  Best Turkish performance:  {best_tr['model_name']} ({best_tr['avg_quality_tr']:.4f})\")\n    print(f\"  Worst Turkish performance: {worst_tr['model_name']} ({worst_tr['avg_quality_tr']:.4f})\")\n\n    # Check if any model responds in English to Turkish prompts\n    for m in models:\n        tr_prompts = [pr for pr in m[\"prompt_results\"] if pr.get(\"language\") == \"tr\"]\n        for pr in tr_prompts:\n            response = pr.get(\"response\", \"\")\n            # Simple heuristic: check for Turkish-specific characters\n            has_turkish_chars = any(c in response for c in \"şğüöçıŞĞÜÖÇİ\")\n            if not has_turkish_chars and response:\n                print(f\"  WARNING: {m['model_name']} may respond in English to Turkish prompts \"\n                      f\"(prompt: {pr['prompt_id']})\")\n\n\nanalyse_turkish_support(models)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ca5y1c7c8i4",
   "source": "## Conclusions and Recommendations\n\n### Performance Summary\n\n| Model | Speed | Quality | Turkish | Memory | Recommendation |\n|-------|-------|---------|---------|--------|----------------|\n| **GPT-4o-mini** | Fastest (1.8s) | Best (0.87) | Best (0.81) | None (API) | Best overall quality; requires external API |\n| **Mistral 7B** | Moderate (4.2s) | Good (0.71) | Fair (0.59) | 5.3 GB | Best self-hosted balance of speed and quality |\n| **LLaMA 3 8B** | Slow (5.2s) | Good (0.74) | Good (0.62) | 5.7 GB | Best self-hosted quality; good Turkish support |\n| **Phi-3 Mini** | Fast (2.9s) | Fair (0.65) | Poor (0.49) | 3.3 GB | Best for low-resource environments |\n\n### Key Findings\n\n1. **Quality vs. Speed Trade-off**: GPT-4o-mini dominates on both quality and speed,\n   but requires an external API. Among self-hosted models, LLaMA 3 8B offers the best\n   quality while Phi-3 Mini is the fastest with the lowest memory footprint.\n\n2. **Turkish Language Gap**: All self-hosted models show a significant quality drop\n   for Turkish prompts (0.15-0.22 gap). Phi-3 Mini particularly struggles, sometimes\n   responding in English to Turkish prompts. GPT-4o-mini has the smallest gap (0.08).\n\n3. **Memory Considerations**: On a 32GB node, running Mistral 7B or LLaMA 3 8B\n   requires approximately 5-6 GB of RAM. Phi-3 Mini is viable on nodes with limited\n   resources (< 4 GB for the model).\n\n4. **Recommendation**: For the AI Stack deployment:\n   - Use **Mistral 7B** as the default self-hosted model (best speed/quality ratio).\n   - Use **LLaMA 3 8B** when Turkish language quality is important.\n   - Consider **QLoRA fine-tuning** on Turkish data to close the EN-TR quality gap.\n   - Keep **GPT-4o-mini** as an optional external fallback for high-quality requirements.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}