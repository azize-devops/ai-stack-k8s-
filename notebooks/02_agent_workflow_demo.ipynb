{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wr5sr7yjfna",
   "source": "# AI Agent Workflow Demo\n\nThis notebook demonstrates two distinct AI agent architectures deployed on our\nKubernetes AI Stack:\n\n1. **CrewAI Research Pipeline** -- A multi-agent system where three specialised agents\n   (Researcher, Writer, Reviewer) collaborate to produce a research report.\n2. **LangGraph Stateful Workflow** -- A graph-based workflow with conditional routing\n   that classifies queries and decides whether to use RAG retrieval or direct generation.\n\nBoth systems use OpenAI-compatible APIs backed by LocalAI, making them fully\nself-hosted and model-agnostic.\n\n> **Note:** The agent code lives in `agents/crewai-research-agent/` and\n> `agents/langgraph-workflow/` respectively.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "leok1115oh",
   "source": "\"\"\"\nImport Dependencies\n-------------------\nWe import CrewAI and LangGraph components. These are also used in the actual\nagent services deployed to Kubernetes. The LLM backend is configured via\nenvironment variables pointing to the LocalAI service.\n\"\"\"\n\nimport os\nimport json\nfrom pprint import pprint\n\n# Set environment variables for the LLM backend (LocalAI in Kubernetes)\n# In production these come from Kubernetes ConfigMaps/Secrets.\nos.environ.setdefault(\"LLM_BASE_URL\", \"http://localai.ai-stack.svc.cluster.local:8080/v1\")\nos.environ.setdefault(\"LLM_API_KEY\", \"sk-no-key-required\")\nos.environ.setdefault(\"LLM_MODEL_NAME\", \"gpt-3.5-turbo\")\nos.environ.setdefault(\"LLM_TEMPERATURE\", \"0.7\")\nos.environ.setdefault(\"LLM_MAX_TOKENS\", \"4096\")\nos.environ.setdefault(\"QDRANT_URL\", \"http://qdrant.ai-stack.svc.cluster.local:6333\")\n\n# CrewAI imports\ntry:\n    from crewai import Agent, Task, Crew, Process\n    from langchain_openai import ChatOpenAI\n    CREWAI_AVAILABLE = True\n    print(\"CrewAI and LangChain loaded successfully\")\nexcept ImportError as e:\n    CREWAI_AVAILABLE = False\n    print(f\"CrewAI not available: {e}\")\n    print(\"Install with: pip install crewai langchain-openai\")\n\n# LangGraph imports\ntry:\n    from langgraph.graph import END, START, StateGraph\n    LANGGRAPH_AVAILABLE = True\n    print(\"LangGraph loaded successfully\")\nexcept ImportError as e:\n    LANGGRAPH_AVAILABLE = False\n    print(f\"LangGraph not available: {e}\")\n    print(\"Install with: pip install langgraph\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y2s5hlva63d",
   "source": "## CrewAI Research Pipeline\n\nThe CrewAI pipeline uses three specialised agents that collaborate sequentially:\n\n| Agent | Role | Responsibility |\n|-------|------|----------------|\n| **Researcher** | Senior Research Analyst | Gather and analyse information on the topic |\n| **Writer** | Technical Writer | Draft a structured report from the research |\n| **Reviewer** | Quality Reviewer | Review the report for accuracy and completeness |\n\nThe pipeline is configured via `config.yaml` and uses `langchain_openai.ChatOpenAI`\nas the LLM backend, pointed at our LocalAI instance.\n\n```\nResearch Task --> Writing Task (context: research) --> Review Task (context: research + writing)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zljdkde589a",
   "source": "# ---------------------------------------------------------------------------\n# CrewAI Agent Creation and Crew Execution\n# ---------------------------------------------------------------------------\n# This mirrors the code in agents/crewai-research-agent/agents.py and tasks.py\n\n# Research topic for this demo\nRESEARCH_TOPIC = \"Best practices for deploying LLMs on Kubernetes with limited GPU resources\"\n\nif CREWAI_AVAILABLE:\n    # Step 1: Configure the shared LLM (pointed at LocalAI)\n    llm = ChatOpenAI(\n        model=os.getenv(\"LLM_MODEL_NAME\", \"gpt-3.5-turbo\"),\n        base_url=os.getenv(\"LLM_BASE_URL\", \"http://localai:8080/v1\"),\n        api_key=os.getenv(\"LLM_API_KEY\", \"sk-no-key-required\"),\n        temperature=0.7,\n        max_tokens=4096,\n        request_timeout=120,\n    )\n\n    # Step 2: Create the three specialised agents\n    researcher = Agent(\n        role=\"Senior Research Analyst\",\n        goal=\"Conduct thorough research and provide comprehensive analysis\",\n        backstory=(\n            \"You are an experienced research analyst specialising in AI/ML \"\n            \"infrastructure and Kubernetes deployments. You excel at finding \"\n            \"and synthesising technical information.\"\n        ),\n        allow_delegation=False,\n        verbose=True,\n        llm=llm,\n    )\n\n    writer = Agent(\n        role=\"Technical Writer\",\n        goal=\"Transform research findings into clear, well-structured reports\",\n        backstory=(\n            \"You are a skilled technical writer with deep knowledge of cloud-native \"\n            \"technologies. You create documentation that is both technically accurate \"\n            \"and accessible to a broad audience.\"\n        ),\n        allow_delegation=False,\n        verbose=True,\n        llm=llm,\n    )\n\n    reviewer = Agent(\n        role=\"Quality Reviewer\",\n        goal=\"Ensure reports are accurate, complete, and well-organised\",\n        backstory=(\n            \"You are a meticulous quality reviewer who checks technical content \"\n            \"for accuracy, completeness, and clarity. You provide constructive \"\n            \"feedback and identify gaps in reasoning.\"\n        ),\n        allow_delegation=False,\n        verbose=True,\n        llm=llm,\n    )\n\n    # Step 3: Define tasks with context chaining\n    research_task = Task(\n        description=(\n            f\"Research the topic: '{RESEARCH_TOPIC}'. Gather key findings, \"\n            \"best practices, and real-world examples. Focus on quantisation \"\n            \"techniques (GGML, GPTQ, AWQ), memory optimisation, and orchestration \"\n            \"patterns for self-hosted LLM inference.\"\n        ),\n        expected_output=(\n            \"A structured research brief with key findings, data points, \"\n            \"and references organised by subtopic.\"\n        ),\n        agent=researcher,\n    )\n\n    writing_task = Task(\n        description=(\n            f\"Write a technical report on '{RESEARCH_TOPIC}' based on the \"\n            \"research findings. Include an executive summary, detailed sections, \"\n            \"and actionable recommendations.\"\n        ),\n        expected_output=(\n            \"A well-structured technical report in markdown format with \"\n            \"headings, bullet points, and a clear conclusion.\"\n        ),\n        agent=writer,\n        context=[research_task],  # Writer receives researcher's output\n    )\n\n    review_task = Task(\n        description=(\n            \"Review the technical report for accuracy, completeness, and clarity. \"\n            \"Check that all claims are supported and the recommendations are practical. \"\n            \"Provide a final quality assessment.\"\n        ),\n        expected_output=(\n            \"A reviewed report with quality score, identified issues, and \"\n            \"the final approved version of the report.\"\n        ),\n        agent=reviewer,\n        context=[research_task, writing_task],  # Reviewer sees both\n    )\n\n    # Step 4: Assemble the Crew with sequential execution\n    crew = Crew(\n        agents=[researcher, writer, reviewer],\n        tasks=[research_task, writing_task, review_task],\n        process=Process.sequential,  # Tasks run in order\n        verbose=True,\n    )\n\n    print(\"CrewAI pipeline assembled successfully!\")\n    print(f\"  Topic:    {RESEARCH_TOPIC}\")\n    print(f\"  Agents:   {len(crew.agents)}\")\n    print(f\"  Tasks:    {len(crew.tasks)}\")\n    print(f\"  Process:  Sequential\")\n    print(f\"  LLM:      {os.getenv('LLM_MODEL_NAME')}\")\n    print(f\"\\nTo execute: result = crew.kickoff()\")\n    print(\"(Execution requires a running LocalAI instance)\")\n\n    # Uncomment to actually run the crew (requires LocalAI to be reachable):\n    # result = crew.kickoff()\n    # print(result)\n\nelse:\n    print(\"CrewAI is not installed. Showing the pipeline structure:\")\n    print()\n    print(\"Pipeline: Researcher --> Writer --> Reviewer\")\n    print(f\"Topic:    {RESEARCH_TOPIC}\")\n    print()\n    print(\"Agents:\")\n    print(\"  1. Senior Research Analyst -- gathers information on the topic\")\n    print(\"  2. Technical Writer -- drafts a structured report from research\")\n    print(\"  3. Quality Reviewer -- validates accuracy and completeness\")\n    print()\n    print(\"Install CrewAI to run: pip install crewai langchain-openai\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9c2f5c97gzo",
   "source": "## LangGraph Stateful Workflow\n\nThe LangGraph workflow implements an intelligent query router with four nodes:\n\n```\nSTART --> classify_query --> [conditional routing]\n                              |\n                              +-- route=\"rag\" -----> retrieve_context --+\n                              |                                         |\n                              +-- route=\"direct\" --------------------+  |\n                                                                     |  |\n                                                                     v  v\n                                                               generate_response\n                                                                     |\n                                                                     v\n                                                               check_quality\n                                                                     |\n                                                                     v\n                                                                    END\n```\n\n**Key concepts:**\n- **State**: A `TypedDict` (`WorkflowState`) that flows through the graph with fields for\n  query, classification, context, response, route, and quality metrics.\n- **Nodes**: Pure functions that receive state and return partial state updates.\n- **Conditional edges**: The routing decision determines whether to fetch context from\n  Qdrant (RAG path) or answer directly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bf3fwps18nu",
   "source": "# ---------------------------------------------------------------------------\n# LangGraph Workflow Construction and Execution\n# ---------------------------------------------------------------------------\n# This mirrors the code in agents/langgraph-workflow/graph.py, state.py, and nodes.py\n\nfrom typing import TypedDict, Any, Annotated\n\nif LANGGRAPH_AVAILABLE:\n\n    # Step 1: Define the state schema (mirrors state.py)\n    class WorkflowState(TypedDict, total=False):\n        \"\"\"Typed state flowing through the LangGraph workflow.\"\"\"\n        query: str              # Original user question\n        classification: str     # \"factual\", \"analytical\", or \"creative\"\n        context: list[str]      # Retrieved context chunks (RAG path only)\n        response: str           # Generated answer\n        route: str              # \"rag\" or \"direct\"\n        quality_score: int      # 0-100 quality score\n        quality_passed: bool    # Whether the response passed quality gate\n\n    # Step 2: Define node functions (simplified versions of nodes.py)\n    def classify_query(state: WorkflowState) -> dict[str, Any]:\n        \"\"\"Classify the query type and determine routing.\"\"\"\n        query = state[\"query\"]\n        # In production, this calls the LLM for classification.\n        # Here we use a simplified heuristic for demonstration.\n        factual_keywords = [\"what\", \"define\", \"explain\", \"how does\", \"describe\"]\n        is_factual = any(kw in query.lower() for kw in factual_keywords)\n\n        classification = \"factual\" if is_factual else \"analytical\"\n        # Route to RAG for factual queries that benefit from context\n        route = \"rag\" if is_factual else \"direct\"\n\n        print(f\"  [classify_query] classification={classification}, route={route}\")\n        return {\"classification\": classification, \"route\": route}\n\n    def retrieve_context(state: WorkflowState) -> dict[str, Any]:\n        \"\"\"Retrieve supporting context from the vector store.\"\"\"\n        query = state[\"query\"]\n        # In production, this queries Qdrant via vector similarity search.\n        # Here we return simulated context for demonstration.\n        context = [\n            f\"Context 1: Relevant information about '{query[:50]}' \"\n            \"from the knowledge base. In production this comes from Qdrant.\",\n            \"Context 2: Supporting data and examples that provide \"\n            \"grounding for the LLM response.\",\n            \"Context 3: Additional reference material related to the topic.\",\n        ]\n        print(f\"  [retrieve_context] Retrieved {len(context)} chunks\")\n        return {\"context\": context}\n\n    def generate_response(state: WorkflowState) -> dict[str, Any]:\n        \"\"\"Generate a response, optionally using retrieved context.\"\"\"\n        query = state[\"query\"]\n        context = state.get(\"context\", [])\n        classification = state.get(\"classification\", \"factual\")\n\n        # In production, this calls the LLM via ChatOpenAI.\n        if context:\n            response = (\n                f\"Based on {len(context)} retrieved sources: \"\n                f\"This is a {classification} query about '{query[:60]}'. \"\n                \"The answer would be generated by the LLM using the \"\n                \"retrieved context for grounding.\"\n            )\n        else:\n            response = (\n                f\"Direct answer for this {classification} query about \"\n                f\"'{query[:60]}'. No external context was needed.\"\n            )\n        print(f\"  [generate_response] Generated {len(response)} chars \"\n              f\"(context_chunks={len(context)})\")\n        return {\"response\": response}\n\n    def check_quality(state: WorkflowState) -> dict[str, Any]:\n        \"\"\"Validate the quality of the generated response.\"\"\"\n        response = state.get(\"response\", \"\")\n        # In production, this calls the LLM to score the response.\n        score = min(85, len(response))  # Simplified scoring\n        passed = score >= 60\n        print(f\"  [check_quality] score={score}, passed={passed}\")\n        return {\"quality_score\": score, \"quality_passed\": passed}\n\n    # Step 3: Build the graph (mirrors graph.py)\n    def route_decision(state: WorkflowState) -> str:\n        \"\"\"Conditional edge: choose next node based on routing decision.\"\"\"\n        route = state.get(\"route\", \"direct\")\n        if route == \"rag\":\n            return \"retrieve_context\"\n        return \"generate_response\"\n\n    graph = StateGraph(WorkflowState)\n\n    # Register nodes\n    graph.add_node(\"classify_query\", classify_query)\n    graph.add_node(\"retrieve_context\", retrieve_context)\n    graph.add_node(\"generate_response\", generate_response)\n    graph.add_node(\"check_quality\", check_quality)\n\n    # Define edges\n    graph.add_edge(START, \"classify_query\")\n    graph.add_conditional_edges(\n        \"classify_query\",\n        route_decision,\n        {\n            \"retrieve_context\": \"retrieve_context\",\n            \"generate_response\": \"generate_response\",\n        },\n    )\n    graph.add_edge(\"retrieve_context\", \"generate_response\")\n    graph.add_edge(\"generate_response\", \"check_quality\")\n    graph.add_edge(\"check_quality\", END)\n\n    # Compile the graph\n    workflow = graph.compile()\n    print(\"LangGraph workflow compiled successfully!\")\n\n    # Step 4: Execute the workflow with sample queries\n    test_queries = [\n        \"What is a Kubernetes Service and how does it work?\",\n        \"Compare the performance of Mistral 7B and LLaMA 3 for code generation tasks.\",\n    ]\n\n    for query in test_queries:\n        print(f\"\\n{'='*65}\")\n        print(f\"Query: {query}\")\n        print('='*65)\n        result = workflow.invoke({\"query\": query})\n        print(f\"\\nResult:\")\n        print(f\"  Classification:  {result.get('classification')}\")\n        print(f\"  Route:           {result.get('route')}\")\n        print(f\"  Context chunks:  {len(result.get('context', []))}\")\n        print(f\"  Quality score:   {result.get('quality_score')}\")\n        print(f\"  Quality passed:  {result.get('quality_passed')}\")\n        print(f\"  Response:        {result.get('response', '')[:120]}...\")\n\nelse:\n    print(\"LangGraph is not installed. Showing the workflow structure:\")\n    print()\n    print(\"Nodes: classify_query -> [route] -> retrieve_context / generate_response -> check_quality\")\n    print()\n    print(\"Install LangGraph to run: pip install langgraph\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rul39u5hije",
   "source": "## Results\n\nThe two agent architectures serve different purposes:\n\n| Feature | CrewAI | LangGraph |\n|---------|--------|-----------|\n| **Architecture** | Multi-agent collaboration | Graph-based state machine |\n| **Execution** | Sequential task pipeline | Conditional node traversal |\n| **State** | Passed via task context | Explicit `TypedDict` schema |\n| **Routing** | Fixed pipeline order | Dynamic conditional edges |\n| **Best for** | Complex multi-step research | Query routing and classification |\n| **LLM calls** | 3+ (one per agent per task) | 2-3 (classify + generate + quality) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "go0f7qibvxl",
   "source": "## Conclusions\n\n### CrewAI Research Pipeline\n- The multi-agent approach excels at complex, multi-step tasks where different\n  expertise is needed at each stage.\n- Sequential processing ensures each agent builds on the previous agent's output.\n- The `config.yaml`-driven design allows tuning agent behaviour without code changes.\n- Trade-off: higher latency (3+ LLM calls) but richer, more polished output.\n\n### LangGraph Stateful Workflow\n- The graph-based approach is ideal for request routing and conditional logic.\n- Conditional edges enable dynamic branching (RAG vs. direct answer).\n- The explicit state schema (`WorkflowState`) makes data flow transparent and testable.\n- The quality gate node provides a safety mechanism to catch low-quality responses.\n- Trade-off: lower latency but less depth per response.\n\n### Deployment on Kubernetes\nBoth agent systems are containerised and deployed as Kubernetes pods, sharing the\nsame LocalAI backend. This architecture allows:\n- Independent scaling of agent services vs. LLM inference.\n- Graceful fallback when Qdrant or LocalAI is temporarily unavailable.\n- Centralised configuration via Kubernetes ConfigMaps and Secrets.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}