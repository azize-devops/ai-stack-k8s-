{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rppv5j4kno",
   "source": "# RAG Pipeline Demo\n\nThis notebook demonstrates the end-to-end **Retrieval-Augmented Generation (RAG)** pipeline\ndeployed on our Kubernetes AI Stack. The pipeline consists of three stages:\n\n1. **Ingest** -- Documents are chunked, embedded via LocalAI, and stored in Qdrant.\n2. **Retrieve** -- User queries are embedded and matched against stored vectors.\n3. **Generate** -- An LLM produces a grounded answer using the retrieved context.\n\n**Architecture:**\n```\nUser Document --> POST /ingest --> Chunk --> Embed (LocalAI) --> Store (Qdrant)\nUser Query   --> POST /query  --> Embed --> Retrieve --> Re-rank --> Generate (LLM)\n```\n\n> **Prerequisites:** The RAG pipeline service, LocalAI, and Qdrant must be running\n> in the `ai-stack` namespace. Use `kubectl get pods -n ai-stack` to verify.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tp4f8br851",
   "source": "\"\"\"\nImport Dependencies\n-------------------\nWe use the `requests` library to interact with the RAG pipeline REST API,\n`openai` for direct LocalAI interaction, and `qdrant_client` for\ninspecting the vector store directly when needed.\n\"\"\"\n\nimport json\nimport time\nfrom pprint import pprint\n\nimport requests\n\n# Optional: direct access to the underlying services\ntry:\n    import openai\n    print(\"openai client available\")\nexcept ImportError:\n    print(\"openai not installed -- direct LLM access unavailable\")\n\ntry:\n    from qdrant_client import QdrantClient\n    print(\"qdrant_client available\")\nexcept ImportError:\n    print(\"qdrant_client not installed -- direct vector DB access unavailable\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "iy99zrgreme",
   "source": "# ---------------------------------------------------------------------------\n# Configuration\n# ---------------------------------------------------------------------------\n# When running inside the cluster, services are reachable via their\n# Kubernetes DNS names. For local development, use kubectl port-forward:\n#   kubectl port-forward -n ai-stack svc/rag-pipeline 8000:8000\n#   kubectl port-forward -n ai-stack svc/qdrant 6333:6333\n\n# RAG Pipeline API endpoint\nRAG_API_URL = \"http://rag-pipeline.ai-stack.svc.cluster.local:8000\"\n\n# For local development (uncomment if using port-forward):\n# RAG_API_URL = \"http://localhost:8000\"\n\n# Qdrant direct access (for inspection)\nQDRANT_URL = \"http://qdrant.ai-stack.svc.cluster.local:6333\"\n\n# Collection name (must match the pipeline configuration)\nCOLLECTION_NAME = \"knowledge_base\"\n\n# Embedding and LLM model names (as configured in the pipeline)\nEMBEDDING_MODEL = \"text-embedding-ada-002\"\nLLM_MODEL = \"gpt-3.5-turbo\"\n\nprint(f\"RAG API:    {RAG_API_URL}\")\nprint(f\"Qdrant:     {QDRANT_URL}\")\nprint(f\"Collection: {COLLECTION_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nwhk4fuewp",
   "source": "## Document Ingestion\n\nThe `/ingest` endpoint accepts a document as raw text, along with optional metadata.\nThe pipeline then:\n\n1. **Chunks** the text into overlapping segments (default: 512 chars, 50 char overlap)\n2. **Embeds** each chunk using the configured embedding model via LocalAI\n3. **Stores** the vectors and metadata in the Qdrant collection\n\nLet's ingest a sample Kubernetes knowledge document.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gwvvj2akfc",
   "source": "# ---------------------------------------------------------------------------\n# Ingest a sample document via POST /ingest\n# ---------------------------------------------------------------------------\n\n# A realistic Kubernetes knowledge document to ingest\nsample_document = \"\"\"\nKubernetes (K8s) is an open-source container orchestration platform originally\ndeveloped by Google. It automates the deployment, scaling, and management of\ncontainerised applications.\n\nKey Concepts:\n- Pod: The smallest deployable unit, consisting of one or more containers that\n  share networking and storage. Pods are ephemeral by design.\n- Deployment: A controller that manages ReplicaSets and provides declarative\n  updates for Pods. It supports rolling updates and rollbacks.\n- Service: An abstraction that defines a logical set of Pods and a policy for\n  accessing them. Types include ClusterIP, NodePort, and LoadBalancer.\n- ConfigMap and Secret: Objects for managing configuration data. ConfigMaps\n  store non-sensitive data, while Secrets handle sensitive information with\n  base64 encoding.\n- PersistentVolumeClaim (PVC): A request for storage by a user. PVCs are\n  bound to PersistentVolumes and provide storage that persists beyond pod\n  lifecycle.\n- Horizontal Pod Autoscaler (HPA): Automatically scales the number of pod\n  replicas based on observed CPU utilisation, memory usage, or custom metrics.\n\nNetworking:\nKubernetes uses a flat networking model where every Pod gets its own IP address.\nServices provide stable endpoints and load balancing across Pod replicas. Ingress\ncontrollers manage external HTTP/HTTPS traffic routing.\n\nStorage:\nThe Container Storage Interface (CSI) allows Kubernetes to support various storage\nbackends including local disks, NFS, Ceph, and cloud provider storage (EBS, GCE PD).\nStatefulSets provide stable storage and network identities for stateful applications.\n\"\"\"\n\n# Prepare the ingest request payload\ningest_payload = {\n    \"text\": sample_document,\n    \"metadata\": {\n        \"source\": \"kubernetes-knowledge-base\",\n        \"category\": \"infrastructure\",\n        \"language\": \"en\",\n    },\n    \"collection_name\": COLLECTION_NAME,\n    \"chunk_size\": 512,\n    \"chunk_overlap\": 50,\n}\n\nprint(\"Ingesting document...\")\nprint(f\"  Document length: {len(sample_document)} characters\")\nprint(f\"  Collection:      {COLLECTION_NAME}\")\nprint(f\"  Chunk size:      {ingest_payload['chunk_size']}\")\nprint(f\"  Chunk overlap:   {ingest_payload['chunk_overlap']}\")\n\ntry:\n    response = requests.post(\n        f\"{RAG_API_URL}/ingest\",\n        json=ingest_payload,\n        timeout=60,\n    )\n    response.raise_for_status()\n    result = response.json()\n\n    print(f\"\\nIngestion successful!\")\n    print(f\"  Document ID:     {result['document_id']}\")\n    print(f\"  Chunks created:  {result['chunks_created']}\")\n    print(f\"  Latency:\")\n    for stage, ms in result[\"latency_ms\"].items():\n        print(f\"    {stage:>12}: {ms:.2f} ms\")\n\nexcept requests.exceptions.ConnectionError:\n    print(\"\\nConnection failed. The RAG pipeline service is not reachable.\")\n    print(\"Make sure the service is running: kubectl get pods -n ai-stack\")\n    # Use a simulated result for demonstration\n    result = {\n        \"document_id\": \"a1b2c3d4e5f67890\",\n        \"chunks_created\": 5,\n        \"collection_name\": COLLECTION_NAME,\n        \"latency_ms\": {\n            \"chunking\": 1.23,\n            \"embedding\": 245.67,\n            \"storage\": 18.45,\n        },\n    }\n    print(\"\\n[Simulated result for demonstration]\")\n    pprint(result)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mvnw7eejo0f",
   "source": "## Querying the Pipeline\n\nNow that we have ingested a document, we can query the RAG pipeline. The `/query` endpoint:\n\n1. **Embeds** the user's question using the same embedding model\n2. **Retrieves** the top-K most similar chunks from Qdrant (default: K=10)\n3. **Re-ranks** candidates using a combined vector + lexical similarity score\n4. **Generates** an answer via the LLM, grounded in the top-N re-ranked chunks (default: N=3)\n\nThe response includes the generated answer, source chunks with scores, and per-stage latency.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "l9wpjti31id",
   "source": "# ---------------------------------------------------------------------------\n# Query the RAG pipeline via POST /query\n# ---------------------------------------------------------------------------\n\nqueries = [\n    \"What is a Kubernetes Pod and how does it relate to containers?\",\n    \"How does Horizontal Pod Autoscaler work?\",\n    \"Explain the difference between ConfigMaps and Secrets in Kubernetes.\",\n]\n\nquery_results = []\n\nfor i, query_text in enumerate(queries, 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"Query {i}: {query_text}\")\n    print('='*70)\n\n    query_payload = {\n        \"query\": query_text,\n        \"collection_name\": COLLECTION_NAME,\n        \"top_k\": 10,       # Number of initial retrieval candidates\n        \"top_n\": 3,         # Number of chunks after re-ranking\n        \"include_sources\": True,\n    }\n\n    try:\n        response = requests.post(\n            f\"{RAG_API_URL}/query\",\n            json=query_payload,\n            timeout=120,\n        )\n        response.raise_for_status()\n        qresult = response.json()\n\n        print(f\"\\nAnswer:\\n{qresult['answer']}\")\n        print(f\"\\nSources ({len(qresult['sources'])} chunks):\")\n        for j, src in enumerate(qresult[\"sources\"], 1):\n            print(f\"  [{j}] score={src['score']:.4f} rerank={src['rerank_score']:.4f}\")\n            print(f\"      {src['text'][:120]}...\")\n\n        print(f\"\\nLatency:\")\n        for stage, ms in qresult[\"latency_ms\"].items():\n            print(f\"  {stage:>18}: {ms:.2f} ms\")\n\n        query_results.append(qresult)\n\n    except requests.exceptions.ConnectionError:\n        print(\"\\nConnection failed -- using simulated result for demonstration.\")\n        simulated = {\n            \"answer\": (\n                \"A Kubernetes Pod is the smallest deployable unit in Kubernetes. \"\n                \"It consists of one or more containers that share networking and storage. \"\n                \"Pods are ephemeral by design and are managed by higher-level controllers \"\n                \"like Deployments.\"\n            ),\n            \"sources\": [\n                {\"text\": \"Pod: The smallest deployable unit...\", \"score\": 0.92, \"rerank_score\": 0.88},\n                {\"text\": \"Kubernetes uses a flat networking model...\", \"score\": 0.78, \"rerank_score\": 0.71},\n            ],\n            \"latency_ms\": {\n                \"query_embedding\": 45.2,\n                \"retrieval\": 12.8,\n                \"reranking\": 0.5,\n                \"generation\": 2340.1,\n            },\n        }\n        print(f\"\\n[Simulated] Answer:\\n{simulated['answer']}\")\n        query_results.append(simulated)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "871xn3r316f",
   "source": "## Results Analysis\n\nLet's analyse the pipeline performance by looking at latency breakdown across\npipeline stages, retrieval quality (vector scores vs. re-rank scores), and\nthe overall response characteristics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9oy7lc4zhaw",
   "source": "# ---------------------------------------------------------------------------\n# Display results in a structured format\n# ---------------------------------------------------------------------------\n\ndef display_latency_table(results: list[dict]) -> None:\n    \"\"\"Print a formatted table of pipeline latency across all queries.\"\"\"\n    print(\"\\nPipeline Latency Breakdown (ms)\")\n    print(\"-\" * 72)\n    header = f\"{'Stage':<20}\"\n    for i in range(len(results)):\n        header += f\"  {'Query ' + str(i+1):>12}\"\n    print(header)\n    print(\"-\" * 72)\n\n    # Collect all stages across all results\n    all_stages = []\n    for r in results:\n        for stage in r.get(\"latency_ms\", {}):\n            if stage not in all_stages:\n                all_stages.append(stage)\n\n    for stage in all_stages:\n        row = f\"{stage:<20}\"\n        for r in results:\n            val = r.get(\"latency_ms\", {}).get(stage, 0)\n            row += f\"  {val:>12.2f}\"\n        print(row)\n\n    # Total row\n    print(\"-\" * 72)\n    row = f\"{'TOTAL':<20}\"\n    for r in results:\n        total = sum(r.get(\"latency_ms\", {}).values())\n        row += f\"  {total:>12.2f}\"\n    print(row)\n    print(\"-\" * 72)\n\n\ndef display_retrieval_quality(results: list[dict]) -> None:\n    \"\"\"Print retrieval score statistics for each query.\"\"\"\n    print(\"\\nRetrieval Quality (Vector Score / Re-rank Score)\")\n    print(\"-\" * 60)\n    for i, r in enumerate(results, 1):\n        sources = r.get(\"sources\", [])\n        if sources:\n            avg_score = sum(s[\"score\"] for s in sources) / len(sources)\n            avg_rerank = sum(s[\"rerank_score\"] for s in sources) / len(sources)\n            print(f\"  Query {i}: {len(sources)} sources | \"\n                  f\"avg_vector={avg_score:.4f} | avg_rerank={avg_rerank:.4f}\")\n        else:\n            print(f\"  Query {i}: No sources returned\")\n\n\ndef display_answer_summary(results: list[dict], queries: list[str]) -> None:\n    \"\"\"Print a concise summary of each answer.\"\"\"\n    print(\"\\nAnswer Summary\")\n    print(\"=\" * 70)\n    for i, (q, r) in enumerate(zip(queries, results), 1):\n        answer = r.get(\"answer\", \"N/A\")\n        # Truncate long answers for readability\n        if len(answer) > 200:\n            answer = answer[:200] + \"...\"\n        print(f\"\\n  Q{i}: {q}\")\n        print(f\"  A{i}: {answer}\")\n    print()\n\n\n# Run the analysis\ndisplay_latency_table(query_results)\ndisplay_retrieval_quality(query_results)\ndisplay_answer_summary(query_results, queries)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "go420m5oxf",
   "source": "## Conclusions\n\n### Pipeline Performance\n- **Chunking** is nearly instantaneous (< 2ms) since it is a simple string operation.\n- **Embedding** is the most variable stage, depending on the LocalAI model and batch size.\n- **Retrieval** from Qdrant is fast (typically < 20ms) thanks to HNSW indexing.\n- **Re-ranking** adds minimal overhead (< 1ms) with the lexical + vector hybrid approach.\n- **Generation** dominates total latency, as the LLM must produce a multi-sentence response.\n\n### Retrieval Quality\n- The two-stage retrieve-then-rerank approach improves result relevance by combining\n  dense vector similarity with lexical overlap signals.\n- Higher `top_k` values increase recall at the cost of more re-ranking computation.\n- The `top_n` parameter controls the context window size fed to the LLM.\n\n### Next Steps\n- Swap the lightweight re-ranker with a cross-encoder model (e.g., `ms-marco-MiniLM`) for higher accuracy.\n- Experiment with hybrid embedding strategies (dense + sparse) via the `EMBEDDING_STRATEGY` config.\n- Add document metadata filtering to scope retrieval to specific knowledge domains.\n- Monitor pipeline latency in production using the `/health` endpoint and Prometheus metrics.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}